import os
import json
import httpx
import asyncio
from typing import Dict, Any, List, Optional, AsyncIterator, Tuple

# -----------------------------------------------------------------------------------
# Environment and constants
# -----------------------------------------------------------------------------------

OPENROUTER_API_KEY = (
    os.getenv("OPENROUTER_API_KEY")
    or os.getenv("OPEN_ROUTER_KEY")
    or os.getenv("OPEN_ROUTER_API_KEY")
    or ""
)
OPENROUTER_URL = "https://openrouter.ai/api/v1/chat/completions"

# Timeouts:
# - Non-streaming requests can finish within DEFAULT_TIMEOUT seconds
# - Streaming requests keep the connection open; we set timeout=None per request
DEFAULT_TIMEOUT = float(os.getenv("OPENROUTER_TIMEOUT_SEC", "60"))

# Optional: Provider preferences for OpenRouter routing
# Example:
# PROVIDER_PREFS = {"order": ["OpenAI", "Anthropic"], "allow_fallbacks": True}
PROVIDER_PREFS: Optional[Dict[str, Any]] = None

# Optional: default headers to forward; adjust as needed
DEFAULT_HEADERS: Dict[str, str] = {
    "Authorization": f"Bearer {OPENROUTER_API_KEY}" if OPENROUTER_API_KEY else "",
    "Content-Type": "application/json",
}

# -----------------------------------------------------------------------------------
# Model registry and tiering
# -----------------------------------------------------------------------------------

# Canonical model list you expose via /api/models
# Assign tiers for access control: "free", "openai", "openrouter", "premium"
# Keep IDs consistent with your frontend and OpenRouter catalog.
MODEL_REGISTRY: List[Dict[str, Any]] = [
    # Free options
    {"id": "meta-llama/llama-3.2-3b-instruct:free", "name": "Llama 3.2 3B (Free)", "tier": "free"},
    {"id": "google/gemini-2.0-flash-001:free", "name": "Gemini 2.0 Flash (Free)", "tier": "free"},
    # OpenRouter standard (requires OpenRouter key)
    {"id": "openai/gpt-4o-mini", "name": "GPT-4o mini", "tier": "openrouter"},
    {"id": "anthropic/claude-3.5-sonnet", "name": "Claude 3.5 Sonnet", "tier": "openrouter"},
    {"id": "mistralai/mistral-small-3.2-24b-instruct", "name": "Mistral Small 24B", "tier": "openrouter"},
    # OpenAI direct (requires OpenAI key on your infra; routed via OpenRouter if configured that way)
    {"id": "openai/gpt-4o", "name": "GPT-4o", "tier": "openai"},
    # Premium bucket for users with both keys (or curated access)
    {"id": "deepseek/deepseek-chat", "name": "DeepSeek Chat", "tier": "premium"},
]

def get_model_tier(model_id: str) -> str:
    for m in MODEL_REGISTRY:
        if m.get("id") == model_id:
            return m.get("tier", "free")
    return "free"

def filter_models_by_tier(models: List[Dict[str, Any]], user_tier: str) -> List[Dict[str, Any]]:
    if user_tier == "premium":
        return models
    if user_tier == "openrouter":
        return [m for m in models if m.get("tier") in ("free", "openrouter")]
    if user_tier == "openai":
        return [m for m in models if m.get("tier") in ("free", "openai")]
    # free
    return [m for m in models if m.get("tier") == "free"]

# -----------------------------------------------------------------------------------
# Backoff helper
# -----------------------------------------------------------------------------------

async def _with_backoff(coro_factory, retries: int = 3, base_delay: float = 0.8, factor: float = 2.0, max_delay: float = 8.0):
    for attempt in range(retries):
        try:
            return await coro_factory()
        except (httpx.HTTPError, asyncio.TimeoutError) as e:
            delay = min(base_delay * (factor ** attempt), max_delay)
            print(f"[ModelService] transient error: {e}; retry {attempt+1}/{retries} in {delay:.2f}s")
            await asyncio.sleep(delay)
    # Final attempt without catching to bubble up
    return await coro_factory()

# -----------------------------------------------------------------------------------
# ModelService
# -----------------------------------------------------------------------------------

class ModelService:
    """
    Provides:
    - chat_completion: non-streaming, returns full text
    - chat_completion_stream: SSE streaming iterator, yields raw text chunks (SSE frames or content deltas, see notes)
    Also exposes model registry helpers for main.py endpoints.
    """

    def __init__(self):
        self._api_key = OPENROUTER_API_KEY

    def get_models(self) -> List[Dict[str, Any]]:
        return MODEL_REGISTRY.copy()

    # ---------------- Non-streaming ----------------

    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        model: str,
        web_search: bool = False,
        provider_prefs: Optional[Dict[str, Any]] = PROVIDER_PREFS,
    ) -> str:
        """
        Sends a non-streaming chat request and returns the final content.
        """
        headers = DEFAULT_HEADERS.copy()
        if not headers.get("Authorization"):
            # Allow app to continue; upstream may reject without key
            print("[ModelService] WARNING: No OpenRouter API key configured")

        payload: Dict[str, Any] = {
            "model": model,
            "messages": messages,
            "stream": False,
        }
        if web_search:
            payload.setdefault("extra_body", {})["web_search"] = True
        if provider_prefs:
            payload["provider"] = provider_prefs

        async def _do():
            async with httpx.AsyncClient(timeout=DEFAULT_TIMEOUT) as client:
                r = await client.post(OPENROUTER_URL, headers=headers, json=payload)
                r.raise_for_status()
                data = r.json()
                # OpenRouter-compatible parse
                content = (data.get("choices") or [{}])[0].get("message", {}).get("content") or ""
                return content

        return await _with_backoff(_do)

    # ---------------- Streaming (SSE) ----------------

    async def chat_completion_stream(
        self,
        messages: List[Dict[str, str]],
        model: str,
        web_search: bool = False,
        provider_prefs: Optional[Dict[str, Any]] = PROVIDER_PREFS,
        extra_headers: Optional[Dict[str, str]] = None,
    ) -> AsyncIterator[str]:
        """
        Async generator yielding SSE text chunks from OpenRouter.

        Yields raw text chunks as received from the network. Each chunk may contain:
          - SSE comment lines (starting with ':') -> keepalive
          - 'data: {json}\n' lines -> payloads with choices[0].delta.content
          - 'data: [DONE]\n' -> end of stream

        The server route in main.py maintains a buffer, splits by '\n', ignores comments,
        parses JSON lines and extracts delta text to forward to the browser.

        Notes:
        - Timeout is None to allow long streams.
        - You can cancel the stream from the frontend by aborting the fetch; httpx will
          raise on disconnect and this iterator will stop.
        """
        headers = DEFAULT_HEADERS.copy()
        if extra_headers:
            headers.update(extra_headers)
        if not headers.get("Authorization"):
            print("[ModelService] WARNING: No OpenRouter API key configured for streaming")

        payload: Dict[str, Any] = {
            "model": model,
            "messages": messages,
            "stream": True,
        }
        if web_search:
            payload.setdefault("extra_body", {})["web_search"] = True
        if provider_prefs:
            payload["provider"] = provider_prefs

        # We don't backoff a live stream; the caller can re-initiate if needed.
        async with httpx.AsyncClient(timeout=None) as client:
            async with client.stream("POST", OPENROUTER_URL, headers=headers, json=payload) as resp:
                resp.raise_for_status()
                async for raw_chunk in resp.aiter_text():
                    if raw_chunk:
                        # We forward raw chunks so the caller can maintain framing per SSE spec.
                        yield raw_chunk

    # ---------------- Optional helpers: token accounting ----------------

    def estimate_tokens(self, messages: List[Dict[str, str]], model: str) -> int:
        """
        Placeholder for token estimation. If you integrate a tokenizer, wire it here.
        For example, using openai-messages-token-helper from Pamela Fox [github.com]:
          https://github.com/pamelafox/openai-messages-token-helper/blob/main/src/openai_messages_token_helper/model_helper.py
        """
        # Implement or call into your tokenizer; returning 0 for now.
        return 0

# -----------------------------------------------------------------------------------
# Re-exports used by main.py
# -----------------------------------------------------------------------------------

__all__ = [
    "ModelService",
    "get_model_tier",
    "filter_models_by_tier",
]